{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "''' Concatenate the contents of all files in the data folder into one single text file named full.txt. '''\n",
    "def concat_files():\n",
    "    with open('full.txt', 'w', encoding='utf8') as f:\n",
    "        for file in os.listdir('data'):\n",
    "            with open(os.path.join('data', file), encoding='utf8') as g:\n",
    "                f.write(g.read())\n",
    "\n",
    "concat_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.SentencePieceTrainer.train(input='full.txt', model_prefix='m_en_de', vocab_size=15000, model_type='unigram', character_coverage=1.0, input_sentence_size=10000000, shuffle_input_sentence=True, max_sentence_length=1000, num_threads=16, unk_id=0, bos_id=1, eos_id=2, pad_id=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.tril(np.ones(attn_shape)).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = subsequent_mask(10)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill mask with -inf\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf'))\n",
    "mask[0, 0, 0] = 1e-9\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(1e-46, dtype=torch.float32) == torch.tensor(0, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(mask, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FibonacciDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "\t''' A toy dataset that generates pairs of fibonaci sequences starting at a random float between -1 and 1.\n",
    "\tThe second sequence is the first sequence shifted by one position to the right.\n",
    "\t '''\n",
    "\n",
    "\tdef __init__(self, block_size=16):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.block_size = block_size\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tstart = np.random.uniform(-1, 1)\n",
    "\t\tfib = [start, start]\n",
    "\t\tfor i in range(self.block_size - 1):\n",
    "\t\t\tfib.append(fib[-1] + fib[-2])\n",
    "\t\tx = torch.tensor(fib[:-1], dtype=torch.float32)\n",
    "\t\ty = torch.tensor(fib[1:], dtype=torch.float32)\n",
    "\t\treturn x, y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn 10000000000000\n",
    "\n",
    "FibonacciDataset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversionDataset(torch.utils.data.Dataset):\n",
    "\t\n",
    "\t''' A toy dataset that generates pairs of sequences, where the second sequence is the first sequence reversed.\n",
    "\t '''\n",
    "\n",
    "\tdef __init__(self, block_size=16):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.block_size = block_size\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t''' Generate a random sequence of float starting from a number between -1 and 1 and its reverse. '''\n",
    "\t\tseq = np.random.uniform(-1, 1, size=(self.block_size,))\n",
    "\t\tx = torch.tensor(seq, dtype=torch.float32)\n",
    "\t\ty = torch.tensor(seq, dtype=torch.float32).flip(dims=(0,))\n",
    "\t\treturn x, y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn 10000000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\t''' Apply learnable token and position embeddings to input tokens. '''\n",
    "\n",
    "\tdef __init__(self, vocab_size: int, emb_dim: int, block_size: int):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
    "\t\tself.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
    "\t\tself.register_buffer('pos_emb_index', torch.arange(block_size))\n",
    "\t\n",
    "\tdef forward(self, x: Tensor):\n",
    "\t\tB, T = x.shape\n",
    "\t\ttok_embd = self.token_embedding_table(x)\n",
    "\t\tpos_embd = self.position_embedding_table(self.pos_emb_index[:T])\n",
    "\t\treturn tok_embd + pos_embd\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "\n",
    "\t''' Multi-head self attention.\n",
    "\tImplements a somewhat optimized version of the self attention by combining the q, k, v projections.\n",
    "\t\n",
    "\tInputs:\n",
    "\t\tx: input tensor of shape (B, T, C)\n",
    "\t\tmask: mask tensor of shape broadcastable to (B, T, T)\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, n_heads: int, emb_dim: int, bias: bool, block_size: int, is_causal: bool = False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.is_causal = is_causal\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.emb_dim = emb_dim\n",
    "\t\t# combine q, k, v projections for efficiency\n",
    "\t\tself.qkv_projection = nn.Linear(emb_dim, 3 * emb_dim, bias=bias)\n",
    "\t\tself.register_buffer('scale', torch.tensor(emb_dim ** -0.5, dtype=torch.float32))\n",
    "\t\tif is_causal:\n",
    "\t\t\tself.register_buffer('causal_mask', torch.tril(torch.ones(1, block_size, block_size, dtype=torch.bool)))\n",
    "\n",
    "\tdef forward(self, x: Tensor, mask: Tensor, return_attentions: bool = False):\n",
    "\t\tB, T, C = x.shape\n",
    "\t\t# proj q, k, v for all heads\n",
    "\t\t# the heads are treated as a batch dimension\n",
    "\t\tq, k, v = self.qkv_projection(x).split(self.emb_dim, dim=2)\n",
    "\t\tq = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\t\tk = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\t\tv = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\t\t# compute attention\n",
    "\t\tatt_weights = (q @ k.transpose(-2, -1)) / self.scale\n",
    "\t\tmask = mask.unsqueeze(1) # apply mask over all heads\n",
    "\t\tif self.is_causal:\n",
    "\t\t\tmask = mask & self.causal_mask[:, :T, :T]\n",
    "\t\tatt_weights = att_weights.masked_fill(mask == 0, -1e9)\n",
    "\t\tatt_weights = F.softmax(att_weights, dim=-1)\n",
    "\t\ty = att_weights @ v\n",
    "\t\t# combine heads\n",
    "\t\ty = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\t\tif return_attentions:\n",
    "\t\t\treturn y, att_weights, v\n",
    "\t\treturn y\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "\n",
    "\t''' Multi-head cross attention.\n",
    "\tImplements a somewhat optimized version of the cross attention by combining the k, v projections.\n",
    "\t\n",
    "\tInputs:\n",
    "\t\tx_q: query tensor of shape (B, T, C)\n",
    "\t\tx_kv: key and value tensor of shape (B, T, C)\n",
    "\t\tmask: mask tensor of shape broadcastable to (B, T, T)\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, n_heads: int, emb_dim: int, bias: bool):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.emb_dim = emb_dim\n",
    "\t\tself.q_projection = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "\t\t# combine k, v projections for efficiency\n",
    "\t\tself.kv_projection = nn.Linear(emb_dim, 2 * emb_dim, bias=bias)\n",
    "\t\tself.register_buffer('scale', torch.tensor(emb_dim ** -0.5, dtype=torch.float32))\n",
    "\n",
    "\tdef forward(self, x_q: Tensor, x_kv: Tensor, mask: Tensor):\n",
    "\t\t# proj query for all heads\n",
    "\t\tB, T, C = x_q.shape\n",
    "\t\tq = self.q_projection(x_q)\n",
    "\t\tq = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\t\t# proj key & value for all heads\n",
    "\t\tB, T, C = x_kv.shape\n",
    "\t\tk, v = self.kv_projection(x_kv).split(self.emb_dim, dim=2)\n",
    "\t\tk = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\t\tv = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\t\t# compute attention\n",
    "\t\tatt_weights = (q @ k.transpose(-2, -1)) / self.scale\n",
    "\t\tmask = mask.unsqueeze(1) # apply mask over all heads\n",
    "\t\tatt_weights = att_weights.masked_fill(mask == 0, -1e9)\n",
    "\t\tatt_weights = F.softmax(att_weights, dim=-1)\n",
    "\t\ty = att_weights @ v\n",
    "\t\t# combine heads\n",
    "\t\ty = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\t\treturn y\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "\tdef __init__(self, emb_dim: int, dropout: float = 0.0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(emb_dim, 4 * emb_dim),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(4 * emb_dim, emb_dim),\n",
    "\t\t)\n",
    "\t\tif dropout:\n",
    "\t\t\tself.net.append(nn.Dropout(dropout))\n",
    "\n",
    "\tdef forward(self, x: Tensor):\n",
    "\t\treturn self.net(x)\n",
    "\t\n",
    "class LMHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, emb_dim: int, vocab_size: int):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.ln = nn.LayerNorm(emb_dim)\n",
    "\t\tself.logits_head = nn.Linear(emb_dim, vocab_size)\n",
    "\t\n",
    "\tdef forward(self, x: Tensor):\n",
    "\t\tx = self.ln(x)\n",
    "\t\tx = self.logits_head(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.331: : 11049it [00:42, 261.28it/s]                        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m x_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(block_size)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     28\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> 29\u001b[0m pred \u001b[39m=\u001b[39m mha(x, x_mask)\n\u001b[0;32m     30\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(pred, y)\n\u001b[0;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[31], line 49\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, x, mask, return_attentions)\u001b[0m\n\u001b[0;32m     47\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(B, T, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[39m# compute attention\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m att_weights \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale\n\u001b[0;32m     50\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m# apply mask over all heads\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_causal:\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1256\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_is_full_backward_hook\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_full_backward_hook \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m   1257\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1258\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# train a single multi-head attention layer on the reversion dataset, using mse loss and adamw optimizer\n",
    "n_steps = 10000\n",
    "batch_size = 128\n",
    "block_size = 32\n",
    "n_heads = 1\n",
    "emb_dim = 1\n",
    "lr = 1e-2\n",
    "\n",
    "mha = MultiHeadSelfAttention(n_heads, emb_dim, False, block_size, False).cuda()\n",
    "\n",
    "# init ds & dl\n",
    "ds = ReversionDataset(block_size)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# init optim\n",
    "optimizer = torch.optim.AdamW(mha.parameters(), lr=lr)\n",
    "\n",
    "# training loop, using tqdm and showing loss at every step\n",
    "pbar = tqdm(dl, total=n_steps)\n",
    "i = 0\n",
    "for x, y in pbar:\n",
    "\tif i > n_steps:\n",
    "\t\tbreak\n",
    "\tx = x.unsqueeze(-1).cuda()\n",
    "\tx_mask = torch.ones(block_size).cuda()\n",
    "\ty = y.unsqueeze(-1).cuda()\n",
    "\tpred = mha(x, x_mask)\n",
    "\tloss = F.mse_loss(pred, y)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\toptimizer.zero_grad()\n",
    "\tpbar.set_description(f'loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 8.159: : 13750it [00:31, 439.38it/s]                        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m x_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(block_size)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     28\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> 29\u001b[0m pred, atten_weights \u001b[39m=\u001b[39m mha(x, x, x)\n\u001b[0;32m     30\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(pred, y)\n\u001b[0;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1148\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m value:\n\u001b[0;32m   1147\u001b[0m     \u001b[39mif\u001b[39;00m query \u001b[39mis\u001b[39;00m key:\n\u001b[1;32m-> 1148\u001b[0m         query \u001b[39m=\u001b[39m key \u001b[39m=\u001b[39m value \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m   1149\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1150\u001b[0m         query, key \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m (query, key)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# train a single multi-head attention layer on the reversion dataset, using mse loss and adamw optimizer\n",
    "n_steps = 10000\n",
    "batch_size = 128\n",
    "block_size = 128\n",
    "n_heads = 1\n",
    "emb_dim = 1\n",
    "lr = 1e-2\n",
    "\n",
    "mha = torch.nn.MultiheadAttention(emb_dim, n_heads, dropout=0.0, bias=False, batch_first=True).cuda()\n",
    "\n",
    "# init ds & dl\n",
    "ds = ReversionDataset(block_size)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# init optim\n",
    "optimizer = torch.optim.AdamW(mha.parameters(), lr=lr)\n",
    "\n",
    "# training loop, using tqdm and showing loss at every step\n",
    "pbar = tqdm(dl, total=n_steps)\n",
    "i = 0\n",
    "for x, y in pbar:\n",
    "\tif i > n_steps:\n",
    "\t\tbreak\n",
    "\tx = x.unsqueeze(-1).cuda()\n",
    "\tx_mask = torch.ones(block_size).cuda()\n",
    "\ty = y.unsqueeze(-1).cuda()\n",
    "\tpred, atten_weights = mha(x, x, x)\n",
    "\tloss = F.mse_loss(pred, y)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\toptimizer.zero_grad()\n",
    "\tpbar.set_description(f'loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\t -0.6607863 -0.7366808 0.6102782 -0.024440741 0.34208342 -0.26214957 0.0769597 0.95468456 0.68525946 -0.39543304 0.19278158 0.49334908 -0.47125027 -0.7380721 0.38035497 -0.5687581\n",
      "y\t -0.5687581 0.38035497 -0.7380721 -0.47125027 0.49334908 0.19278158 -0.39543304 0.68525946 0.95468456 0.0769597 -0.26214957 0.34208342 -0.024440741 0.6102782 -0.7366808 -0.6607863\n",
      "y_pred\t 0.0059240647 0.0074955337 -0.020324573 -0.0072353296 -0.014798902 -0.002323348 -0.009329136 -0.027408224 -0.02186798 0.00043283403 -0.011719506 -0.017916426 0.0020012744 0.0075243376 -0.015587892 0.0040190667\n",
      "v\t -0.6717513 -0.74890524 0.6204051 -0.02484631 0.34775993 -0.26649967 0.078236766 0.9705265 0.6966306 -0.40199482 0.1959806 0.50153565 -0.47907016 -0.7503196 0.38666657 -0.578196\n"
     ]
    }
   ],
   "source": [
    "# test the mha layer on the fibonacci dataset\n",
    "ds = ReversionDataset(16)\n",
    "x, y = ds[0]\n",
    "x = x.unsqueeze(0).unsqueeze(-1).cuda()\n",
    "x_mask = torch.ones(x.shape[1], dtype=torch.long).cuda()\n",
    "y_pred, attn, v = mha(x, x_mask, True)\n",
    "print('x\\t', *x.view(16).cpu().numpy())\n",
    "print('y\\t', *y.view(16).numpy())\n",
    "print('y_pred\\t', *y_pred.detach().view(16).cpu().numpy())\n",
    "print('v\\t', *v.detach().view(16).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the attention weights as heatmap, using seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(attn[0, 0].detach().cpu().numpy(), cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.372:  47%|     | 4707/10000 [00:14<00:16, 329.86it/s]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(dl, total\u001b[39m=\u001b[39mn_steps)\n\u001b[0;32m     13\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m pbar:\n\u001b[0;32m     15\u001b[0m \t\u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m n_steps:\n\u001b[0;32m     16\u001b[0m \t\t\u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Chen Zerui\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m, in \u001b[0;36mFibonacciDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \tfib\u001b[39m.\u001b[39mappend(fib[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m fib[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(fib[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 17\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(fib[\u001b[39m1\u001b[39;49m:], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m x, y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_steps = 10000\n",
    "batch_size = 128\n",
    "mha = MultiHeadSelfAttention(1, 1, False, 16, True).cuda()\n",
    "\n",
    "# train a single mha layer on the fibonacci dataset for n_steps, using mse loss and adamw optimizer\n",
    "ds = FibonacciDataset(16)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "optimizer = torch.optim.AdamW(mha.parameters(), lr=5e-3)\n",
    "\n",
    "pbar = tqdm(dl, total=n_steps)\n",
    "i = 0\n",
    "for x, y in pbar:\n",
    "\tif i > n_steps:\n",
    "\t\tbreak\n",
    "\tx = x.unsqueeze(-1).cuda()\n",
    "\tx_mask = subsequent_mask(x.shape[1]).cuda()\n",
    "\ty = y.unsqueeze(-1).cuda()\n",
    "\tpred = mha(x, x_mask)\n",
    "\tloss = F.mse_loss(pred, y)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\toptimizer.zero_grad()\n",
    "\tpbar.set_description(f'loss: {loss.item():.3f}')\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3462],\n",
       "        [-0.3910],\n",
       "        [ 1.6179]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.qkv_projection.weight.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TranslationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TranslationDataset('data/multi30k/train.en', 'data/multi30k/train.de', 'data/multi30k/m_en_de.model', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
