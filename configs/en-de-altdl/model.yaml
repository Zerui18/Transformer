block_size: 256
src_vocab_size: 19214
tgt_vocab_size: 10837
n_blocks: 3
n_heads: 8 # Number of heads in multi-head attention, each head has dim = emb_dim/n_heads
emb_dim: 512
dropout: 0.1 # Applies to: post-attention, post-linear
bias: false # Applies to: qkv projections in attention layers
weight_tying: false
use_grad_ckpt: false