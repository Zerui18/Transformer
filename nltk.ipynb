{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = SentencePieceProcessor(model_file='/home/chen/Desktop/ML/Transformer/data/multi30k/m_en_de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Tokenizer(abc.ABC):\n",
    "\n",
    "\tdef __init__(self, tokenize_args: dict = {}, detokenize_args: dict = {}):\n",
    "\t\tself.tokenize_args = tokenize_args\n",
    "\t\tself.detokenize_args = detokenize_args\n",
    "\t\tpass\n",
    "\n",
    "\tdef tokenize(self, sentence: str) -> list:\n",
    "\t\treturn self._tokenize(sentence, self.tokenize_args)\n",
    "\t\n",
    "\tdef detokenize(self, tokens: list) -> str:\n",
    "\t\treturn self._detokenize(tokens, self.detokenize_args)\n",
    "\n",
    "\t@abc.abstractmethod\n",
    "\tdef _tokenize(self, sentence: str, tokenize_args: dict) -> list:\n",
    "\t\tpass\n",
    "\t\n",
    "\t@abc.abstractmethod\n",
    "\tdef _detokenize(self, tokens: list, detokenize_args: dict) -> str:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePieceTokenizer(Tokenizer):\n",
    "\n",
    "\tdef __init__(self, sp_model_path: str, tokenize_args: dict = {}, detokenize_args: dict = {}):\n",
    "\t\tself.sp_model = SentencePieceProcessor(model_file=sp_model_path)\n",
    "\t\tsuper().__init__(tokenize_args, detokenize_args)\n",
    "\n",
    "\tdef _tokenize(self, sentence: str, tokenize_args: dict) -> list:\n",
    "\t\treturn self.sp_model.encode(sentence, **tokenize_args)\n",
    "\t\n",
    "\tdef _detokenize(self, tokens: list, detokenize_args: dict) -> str:\n",
    "\t\treturn self.sp_model.decode(tokens, **detokenize_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "BOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "PAD_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu_score(pred: Tensor, reference: Tensor, tokenizer: Tokenizer) -> float:\n",
    "\t'''\n",
    "\tCalculate BLEU-4 score of a single prediction and reference pair by detokenizing and using NLTK.\n",
    "\n",
    "\tArgs:\n",
    "\t\t`pred`: Tensor<Long>[T] predicted tokens.\n",
    "\t\t`reference`: Tensor<Long>[T] reference tokens.\n",
    "\t\n",
    "\tReturns:\n",
    "\t\t`score`: float BLEU-4 score in [0, 1].\n",
    "\t'''\n",
    "\t# clean pred and reference\n",
    "\t# remove all <unk> and <pad> tokens\n",
    "\tpred = pred[pred != UNK_IDX]\n",
    "\tpred = pred[pred != PAD_IDX]\n",
    "\treference = reference[reference != UNK_IDX]\n",
    "\treference = reference[reference != PAD_IDX]\n",
    "\tprint('cleaned predictio:', pred)\n",
    "\tprint('cleaned reference:', reference)\n",
    "\t# detokenize\n",
    "\tpred = tokenizer.detokenize(pred.tolist())\n",
    "\treference = tokenizer.detokenize(reference.tolist())\n",
    "\tprint('detokenized predictio:', pred)\n",
    "\tprint('detokenized reference:', reference)\n",
    "\t# re-tokenize\n",
    "\tpred = tokenizer.tokenize(pred)\n",
    "\treference = tokenizer.tokenize(reference)\n",
    "\tprint('re-tokenized predictio:', pred)\n",
    "\tprint('re-tokenized reference:', reference)\n",
    "\t# calculate BLEU-4 score\n",
    "\treturn sentence_bleu([reference], pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "reference = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokenizer = SentencePieceTokenizer('/home/chen/Desktop/ML/Transformer/data/multi30k/m_en_de.model', tokenize_args={'enable_sampling': True, 'alpha': 0.1})\n",
    "reference_tokenizer = SentencePieceTokenizer('/home/chen/Desktop/ML/Transformer/data/multi30k/m_en_de.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Sitting casually in a public place, a girl reads holding the book open with her hand on which is a butterfly ring.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_tokens: [14482, 168, 8261, 18, 5, 2281, 3555, 14930, 5, 31, 207, 2498, 14900, 14910, 3655, 54, 1574, 1510, 14900, 14916, 86, 32, 7, 32, 10, 14911, 62, 2427, 65, 5, 9247, 144, 14903, 14902, 14913, 14917]\n",
      "reference_tokens: [14482, 168, 8261, 18, 5, 2281, 3555, 14930, 5, 221, 2498, 339, 54, 1574, 1510, 87, 241, 543, 62, 2427, 65, 5, 9247, 4291, 14917]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = sample_tokenizer.tokenize(sentence)\n",
    "reference_tokens = reference_tokenizer.tokenize(sentence)\n",
    "\n",
    "print('sample_tokens:', sample_tokens)\n",
    "print('reference_tokens:', reference_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens = torch.tensor(sample_tokens)\n",
    "reference_tokens = torch.tensor(reference_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned predictio: tensor([14482,   168,  8261,    18,     5,  2281,  3555, 14930,     5,    31,\n",
      "          207,  2498, 14900, 14910,  3655,    54,  1574,  1510, 14900, 14916,\n",
      "           86,    32,     7,    32,    10, 14911,    62,  2427,    65,     5,\n",
      "         9247,   144, 14903, 14902, 14913, 14917])\n",
      "cleaned reference: tensor([14482,   168,  8261,    18,     5,  2281,  3555, 14930,     5,   221,\n",
      "         2498,   339,    54,  1574,  1510,    87,   241,   543,    62,  2427,\n",
      "           65,     5,  9247,  4291, 14917])\n",
      "detokenized predictio: Sitting casually in a public place, a girl reads holding the book open with her hand on which is a butterfly ring.\n",
      "detokenized reference: Sitting casually in a public place, a girl reads holding the book open with her hand on which is a butterfly ring.\n",
      "re-tokenized predictio: [14482, 168, 8261, 18, 5, 2281, 3555, 14930, 5, 221, 2498, 339, 54, 1574, 1510, 87, 241, 543, 62, 2427, 65, 5, 9247, 4291, 14917]\n",
      "re-tokenized reference: [14482, 168, 8261, 18, 5, 2281, 3555, 14930, 5, 221, 2498, 339, 54, 1574, 1510, 87, 241, 543, 62, 2427, 65, 5, 9247, 4291, 14917]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bleu_score(sample_tokens, reference_tokens, reference_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer.load_from_checkpoint('/home/chen/Desktop/ML/Transformer/experiments/de-en-v1-multi30k/de-en-v1-sp-nb_6-multi30k/checkpoints/model-epoch=35-step=4000-val_loss=3.03.ckpt').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.translate import TranslationDataset, TranslationDatasetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input files...\n",
      "                                                 src  \\\n",
      "0  Eine Gruppe von Männern lädt Baumwolle auf ein...   \n",
      "1  Ein Mann schläft in einem grünen Raum auf eine...   \n",
      "2  Ein Junge mit Kopfhörern sitzt auf den Schulte...   \n",
      "3  Zwei Männer bauen eine blaue Eisfischerhütte a...   \n",
      "4  Ein Mann mit beginnender Glatze, der eine rote...   \n",
      "5  Eine Frau in einem rotem Mantel, die eine verm...   \n",
      "6  Ein brauner Hund rennt dem schwarzen Hund hint...   \n",
      "7  Ein kleiner Junge mit einem Giants-Trikot schw...   \n",
      "8  Ein Mann telefoniert in einem unaufgeräumten Büro   \n",
      "9  Eine lächelnde Frau mit einem pfirsichfarbenen...   \n",
      "\n",
      "                                                 tgt  \n",
      "0     A group of men are loading cotton onto a truck  \n",
      "1         A man sleeping in a green room on a couch.  \n",
      "2  A boy wearing headphones sits on a woman's sho...  \n",
      "3  Two men setting up a blue ice fishing hut on a...  \n",
      "4  A balding man wearing a red life jacket is sit...  \n",
      "5  A lady in a red coat, holding a bluish hand ba...  \n",
      "6        A brown dog is running after the black dog.  \n",
      "7  A young boy wearing a Giants jersey swings a b...  \n",
      "8  A man in a cluttered office is using the telep...  \n",
      "9  A smiling woman in a peach tank top stands hol...  \n",
      "Loading sentencepiece models...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "config = TranslationDatasetConfig(\n",
    "\tsrc_sp_model_file='/home/chen/Desktop/ML/Transformer/data/multi30k/m_en_de.model',\n",
    "\ttgt_sp_model_file='/home/chen/Desktop/ML/Transformer/data/multi30k/m_en_de.model',\n",
    "\tsrc_file='/home/chen/Desktop/ML/Transformer/data/multi30k/val.de',\n",
    "\ttgt_file='/home/chen/Desktop/ML/Transformer/data/multi30k/val.en',\n",
    "\tmax_seq_len=128,\n",
    ")\n",
    "\n",
    "dataset = TranslationDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=dataset.get_collate_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerInputBatch(x_src=tensor([[    1,    26,     4, 14901,   320,    70, 14908, 14902,   129, 14902,\n",
      "         11687, 14900, 14909, 14926, 14911, 14906,  1142, 14916,  3581,    63,\n",
      "            12,     6,  4122,     2,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,    27,    73,  1822,    18,    42,   688,   172,   703,     5,\n",
      "         14912, 14920,    12, 14901, 14914,  2659, 14917,     2,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,    27,   284,    61,  5751,   278,    63,   265,  4052,    82,\n",
      "           104, 14917,     2,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,   156,    30, 14926,   248,  4631,   130,  2520,  1582, 14920,\n",
      "          1093, 10062,    63,    42,  7388,  8506,     6,  1913,    63,     2,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,    26, 14903, 14902,    73,    61, 13479,  1433, 14900, 14938,\n",
      "           659,   370, 14930,    20,     7,   130,  2217,  5875, 14916,  1667,\n",
      "           467, 14930,   278,    18,    42,   852,  1182, 14917,     2,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,    90,    51,    89,    18,    42,  1586,  1447, 14930,    20,\n",
      "            71,   130, 12632,   338,  7405,    57,   125, 14914, 14901, 14902,\n",
      "           311, 14900, 14933,    74, 14906,  1478,    18,    42,   400, 12909,\n",
      "           785,    22,   328, 14930,     8, 14919, 14905,   389,   713,   148,\n",
      "          1838, 13742,    18,   142,   680, 14917,     2],\n",
      "        [    1, 14900, 14923,     4,  1209,   226,   911,   213,  1474,    28,\n",
      "           723,   226,  4852, 14917,     2,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,    27,   107,    29,  4138,   284,    61,    42,    92, 14594,\n",
      "         14943, 14932,  6104, 14908, 14906,   120, 14916,   389,   148,  8858,\n",
      "           147,    96,  3982,    18,  2206,   382,     5, 14902, 13744,  5994,\n",
      "         14917,     2,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3]]), x_tgt=tensor([[    1,    24,   294,    76,   275,   132,  6909, 14900, 14915,  2698,\n",
      "            22,    62, 14906, 14908,     5,  1639,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [    1,    24,    75, 14900, 14907,  1373,    15,    18,     5,   438,\n",
      "          1153,    62,     5,  2308, 14917,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [    1,    24,   171, 14924,   181,   763,  2413,   708,    62,     5,\n",
      "           137, 14957, 14907,     8, 14910, 14908,   267,  3442, 14917,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [    1,    59, 14916, 14908,   275,     8,  1026,     4, 14913,   390,\n",
      "             5,   245,   250,   252,  2110,  7636,    62,    37,  6502,   110,\n",
      "            40, 14928,     7,   253,  1003,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [    1,    24,  2891, 14903, 14902, 14913,    75,   181,     5,   249,\n",
      "          3594,   593,    65,   105, 14906,     4, 14913,    18,     5,   570,\n",
      "          1052, 14917,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [    1,    24,   884,    18,     5,   249,  1095, 14930,   279,    15,\n",
      "             5,    17, 14909, 14912,  1161,   543,  1108, 13527,    76,  2156,\n",
      "          4891,     6, 14906, 14930,   699,   651,    54,   176, 14912, 14902,\n",
      "         14911,   414,     5, 11937, 12885, 14917],\n",
      "        [    1,    24,   493,   227,    65,   394, 14902, 14903, 14902, 14913,\n",
      "          2119,    54,   230,   227, 14917,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [    1,    24,   217,   236,   181,     5,    92, 14594,  2238,     8,\n",
      "         14916,  2266,     5,  6532,   147,    96,  1610,   153,    37, 13823,\n",
      "          5693, 14917,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3]]), x_src_mask=tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False]]), x_tgt_mask=tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]]), y_tgt=tensor([[   24,   294,    76,   275,   132,  6909, 14900, 14915,  2698,    22,\n",
      "            62, 14906, 14908,     5,  1639,     2,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [   24,    75, 14900, 14907,  1373,    15,    18,     5,   438,  1153,\n",
      "            62,     5,  2308, 14917,     2,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [   24,   171, 14924,   181,   763,  2413,   708,    62,     5,   137,\n",
      "         14957, 14907,     8, 14910, 14908,   267,  3442, 14917,     2,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [   59, 14916, 14908,   275,     8,  1026,     4, 14913,   390,     5,\n",
      "           245,   250,   252,  2110,  7636,    62,    37,  6502,   110,    40,\n",
      "         14928,     7,   253,  1003,     2,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [   24,  2891, 14903, 14902, 14913,    75,   181,     5,   249,  3594,\n",
      "           593,    65,   105, 14906,     4, 14913,    18,     5,   570,  1052,\n",
      "         14917,     2,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [   24,   884,    18,     5,   249,  1095, 14930,   279,    15,     5,\n",
      "            17, 14909, 14912,  1161,   543,  1108, 13527,    76,  2156,  4891,\n",
      "             6, 14906, 14930,   699,   651,    54,   176, 14912, 14902, 14911,\n",
      "           414,     5, 11937, 12885, 14917,     2],\n",
      "        [   24,   493,   227,    65,   394, 14902, 14903, 14902, 14913,  2119,\n",
      "            54,   230,   227, 14917,     2,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3],\n",
      "        [   24,   217,   236,   181,     5,    92, 14594,  2238,     8, 14916,\n",
      "          2266,     5,  6532,   147,    96,  1610,   153,    37, 13823,  5693,\n",
      "         14917,     2,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3]]))\n"
     ]
    }
   ],
   "source": [
    "for batch in dl:\n",
    "\tprint(batch)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.BOS_IDX, dataset.EOS_IDX, dataset.PAD_IDX, dataset.UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chen/Desktop/ML/Transformer/nltk.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chen/Desktop/ML/Transformer/nltk.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chen/Desktop/ML/Transformer/nltk.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \tsample \u001b[39m=\u001b[39m dataset[i]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/chen/Desktop/ML/Transformer/nltk.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \t\u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msrc:\u001b[39m\u001b[39m'\u001b[39m, sample[\u001b[39m'\u001b[39;49m\u001b[39msrc\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chen/Desktop/ML/Transformer/nltk.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \t\u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtgt:\u001b[39m\u001b[39m'\u001b[39m, sample[\u001b[39m'\u001b[39m\u001b[39mtgt\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tsample = dataset[i]\n",
    "\tx_src, x_tgt, y_tgt = sample\n",
    "\tprint('x_src:', x_src.shape, x_src)\n",
    "\tprint('x_tgt:', x_tgt.shape, x_tgt)\n",
    "\tprint('y_tgt:', y_tgt.shape, y_tgt)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1000),\n",
       " (1, 2000),\n",
       " (2, 1000),\n",
       " (2, 2000),\n",
       " (4, 1000),\n",
       " (4, 2000),\n",
       " (5, 1000),\n",
       " (5, 2000),\n",
       " (6, 1000),\n",
       " (6, 2000),\n",
       " (7, 1000),\n",
       " (7, 2000),\n",
       " (8, 1000),\n",
       " (8, 2000)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_BLOCKS = [1, 2, 4, 5, 6, 7, 8]\n",
    "VOCAB_SIZE = [1000, 2000]\n",
    "\n",
    "list(product(N_BLOCKS, VOCAB_SIZE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
